{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from collections import defaultdict\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_directory(source, destination):\n",
    "    try:\n",
    "        shutil.copytree(source, destination)\n",
    "        print(f\"Directory copied from {source} to {destination}\")\n",
    "    except shutil.Error as e:\n",
    "        print(f\"Directory copy failed: {e}\")\n",
    "    except OSError as e:\n",
    "        print(f\"Directory copy failed: {e}\")\n",
    "\n",
    "\n",
    "def delete_file(file_path):\n",
    "    try:\n",
    "        os.remove(file_path)\n",
    "        print(f\"The file '{file_path}' has been successfully deleted.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File '{file_path}' not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while deleting the file '{file_path}': {e}\")\n",
    "\n",
    "\n",
    "def delete_folder(folder_path):\n",
    "    try:\n",
    "        shutil.rmtree(folder_path)\n",
    "        print(f\"The folder '{folder_path}' has been successfully deleted.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Folder '{folder_path}' not found.\")\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            f\"An error occurred while deleting the folder '{folder_path}': {e}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy source dir to target dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder '../COPY_ACT-LEGISLATION_HTML' not found.\n",
      "Directory copied from ../docs/2023-05-18__ACT-LEGISLATION_HTML to ../COPY_ACT-LEGISLATION_HTML\n"
     ]
    }
   ],
   "source": [
    "main_directory = \"../COPY_ACT-LEGISLATION_HTML\"\n",
    "source_directory = \"../docs/2023-05-18__ACT-LEGISLATION_HTML\"\n",
    "delete_folder(main_directory)\n",
    "copy_directory(source_directory, main_directory)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the logger\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# set the log file\n",
    "log_file = f\"{main_directory}/legislation.log\"\n",
    "handler = logging.FileHandler(log_file)\n",
    "logger.addHandler(handler)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import urllib.parse"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parser funcs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_include_postfix(file_name, doc_type):\n",
    "    if doc_type == 'SECTION':\n",
    "        # Returns the number of capital letters following the section number\n",
    "        post_num_pattern = r'SECT\\s\\d+([A-Z]+)'\n",
    "        post_num_match = re.search(post_num_pattern, file_name)\n",
    "        return len(post_num_match.group(1)) if post_num_match else 0\n",
    "    elif doc_type == 'SCHEDULE':\n",
    "        # Returns the number of capital letters following the schedule number\n",
    "        post_num_pattern = r'SCHEDULE\\s(\\d+)([A-Z]+)'\n",
    "        post_num_match = re.search(post_num_pattern, file_name)\n",
    "        return len(post_num_match.group(2)) if post_num_match else 0\n",
    "\n",
    "\n",
    "def extract_section_details(file_name, directory_name):\n",
    "    base_url = \"https://storage.googleapis.com/law-docs/ACT-LEGISLATION_HTML\"\n",
    "\n",
    "    # match section pattern\n",
    "    sect_pattern = r'SECT\\s(\\d+)([A-Z]*)(.*)([\\d\\.]*)\\.html$'\n",
    "    sect_match = re.search(sect_pattern, file_name)\n",
    "\n",
    "    if sect_match:\n",
    "        sect_num = 's' + sect_match.group(1)\n",
    "        postfix = sect_match.group(2)\n",
    "        num_of_capital_letters = should_include_postfix(file_name, 'SECTION')\n",
    "        if postfix and num_of_capital_letters > 1:\n",
    "            sect_num += postfix[:num_of_capital_letters - 1].lower()\n",
    "            sect_name = postfix[-1]+sect_match.group(3)\n",
    "        else:\n",
    "            sect_name = sect_match.group(2) + sect_match.group(3)\n",
    "        sect_url = f\"{base_url}/{urllib.parse.quote(directory_name)}/{urllib.parse.quote(sect_num)}.html\"\n",
    "        return sect_num, sect_name, sect_url, None\n",
    "\n",
    "    # match schedule pattern\n",
    "    schedule_pattern = r'SCHEDULE (\\d+)([A-Z]*)\\.html$'\n",
    "    schedule_match = re.search(schedule_pattern, file_name)\n",
    "\n",
    "    if schedule_match:\n",
    "        sched_num = 'sch-' + schedule_match.group(1)\n",
    "        postfix = schedule_match.group(2)\n",
    "        num_of_capital_letters = should_include_postfix(file_name, 'SCHEDULE')\n",
    "        if postfix and num_of_capital_letters > 1:\n",
    "            sched_num += postfix[:num_of_capital_letters - 1].lower()\n",
    "            sched_name = 'SCHEDULE ' + schedule_match.group(1) + postfix[-1]\n",
    "        else:\n",
    "            sched_num += postfix.lower()\n",
    "            sched_name = 'SCHEDULE ' + schedule_match.group(1) + postfix\n",
    "        sched_url = f\"{base_url}/{urllib.parse.quote(directory_name)}/{urllib.parse.quote(sched_num)}.html\"\n",
    "        return sched_num, sched_name, sched_url, None\n",
    "    else:\n",
    "        schedule_pattern = r'SCHEDULE\\.html$'\n",
    "        schedule_match = re.search(schedule_pattern, file_name)\n",
    "        if schedule_match:\n",
    "            sched_num = 'sch-0'\n",
    "            sched_name = 'SCHEDULE'\n",
    "            sched_url = f\"{base_url}/{urllib.parse.quote(directory_name)}/{urllib.parse.quote(sched_num)}.html\"\n",
    "            return sched_num, sched_name, sched_url, None\n",
    "        else:\n",
    "            schedule_pattern = r'SCHEDULE\\s(.*)\\.html$'\n",
    "            schedule_match = re.search(schedule_pattern, file_name)\n",
    "            if schedule_match:\n",
    "                sched_num = 'sch-' + schedule_match.group(1)\n",
    "                sched_name = 'SCHEDULE ' + schedule_match.group(1)\n",
    "                sched_url = f\"{base_url}/{urllib.parse.quote(directory_name)}/{urllib.parse.quote(sched_num)}.html\"\n",
    "                return sched_num, sched_name, sched_url, None\n",
    "\n",
    "    # match longtitle pattern\n",
    "    longtitle_pattern = r'LONG TITLE\\.html$'\n",
    "    longtitle_match = re.search(longtitle_pattern, file_name)\n",
    "\n",
    "    if longtitle_match:\n",
    "        sect_url = f\"{base_url}/{urllib.parse.quote(directory_name)}/LONG%20TITLE.html\"\n",
    "        # return \"LONG TITLE\" instead of '0'\n",
    "        return \"LONG TITLE\", 'LONG TITLE', sect_url, None\n",
    "\n",
    "    # match notes pattern\n",
    "    notes_pattern = r'NOTES\\.html$'\n",
    "    notes_match = re.search(notes_pattern, file_name)\n",
    "\n",
    "    if notes_match:\n",
    "        sect_url = f\"{base_url}/{urllib.parse.quote(directory_name)}/NOTES.html\"\n",
    "        return \"NOTES\", 'NOTES', sect_url, None\n",
    "\n",
    "    if not any([sect_match, schedule_match, longtitle_match, notes_match]):\n",
    "        logger.error(f'Unmatched file: {file_name}')\n",
    "        return None, None, None, file_name\n",
    "\n",
    "\n",
    "def get_index_url(directory_name):\n",
    "    base_url = \"https://storage.googleapis.com/law-docs/2023-05-27__ACT-LEGISLATION_HTML\"\n",
    "\n",
    "    index_url = f\"{base_url}/{urllib.parse.quote(directory_name)}/index.html\"\n",
    "    return index_url\n",
    "\n",
    "\n",
    "def rename_files_in_directory(directory):\n",
    "    legislation_name = os.path.basename(directory)\n",
    "    year_match = re.search(r'\\d{4}$', legislation_name)\n",
    "    if year_match:\n",
    "        year = year_match.group(0)\n",
    "    else:\n",
    "        logger.error(f\"Could not get year for {legislation_name}\")\n",
    "        year = None\n",
    "    sections = []\n",
    "    unmatched_files = []\n",
    "    for file in os.listdir(directory):\n",
    "        f = os.path.splitext(file)[0]\n",
    "        if f == legislation_name:\n",
    "            os.rename(os.path.join(directory, file),\n",
    "                      os.path.join(directory, \"index.html\"))\n",
    "            sections.append({\n",
    "                \"section_name\": \"index\",\n",
    "                \"section_order\": \"index\",\n",
    "                \"section_url\": get_index_url(legislation_name)\n",
    "            })\n",
    "        else:\n",
    "            sect_num, sect_name, sect_url, unmatched_file = extract_section_details(\n",
    "                file, legislation_name)\n",
    "            if sect_num is not None and sect_name is not None:\n",
    "                new_file_name = f\"{sect_num}.html\"\n",
    "\n",
    "                os.rename(os.path.join(directory, file),\n",
    "                          os.path.join(directory, new_file_name))\n",
    "\n",
    "                if sect_num.startswith('sch'):  # split the schedule num\n",
    "                    _, section_order = sect_num.split('-')\n",
    "                elif sect_num.startswith('s'):  # split the section num\n",
    "                    _, section_order = sect_num[0], sect_num[1:]\n",
    "                else:  # for \"LONG TITLE\" and \"NOTES\"\n",
    "                    section_order = sect_num\n",
    "\n",
    "                sections.append({\n",
    "                    \"section_name\": sect_name,\n",
    "                    \"section_order\": section_order,\n",
    "                    \"section_url\": sect_url\n",
    "                })\n",
    "            elif unmatched_file:\n",
    "                unmatched_files.append(unmatched_file)\n",
    "            else:\n",
    "                logger.error(f\"Could not extract section details for {file}\")\n",
    "    return {\"legislation\": legislation_name, \"year\": year, \"sections\": sections, \"unmatched_files\": unmatched_files}\n",
    "\n",
    "\n",
    "def create_legislation_json(directory):\n",
    "    legislation_details = rename_files_in_directory(directory)\n",
    "    return legislation_details"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process all directories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_directories(main_directory):\n",
    "    all_legislations = []\n",
    "    missing_sections = {}\n",
    "    original_file_counts = {}\n",
    "\n",
    "    for directory in os.listdir(main_directory):\n",
    "        directory_path = os.path.join(main_directory, directory)\n",
    "        if os.path.isdir(directory_path):\n",
    "            original_file_counts[directory] = len(os.listdir(directory_path))\n",
    "\n",
    "    # Process directories and count sections in JSON\n",
    "    for directory in os.listdir(main_directory):\n",
    "        directory_path = os.path.join(main_directory, directory)\n",
    "        if os.path.isdir(directory_path):\n",
    "            legislation_dict = create_legislation_json(directory_path)\n",
    "            section_count = len(legislation_dict[\"sections\"])\n",
    "            unmatched_files = legislation_dict[\"unmatched_files\"]\n",
    "\n",
    "            if original_file_counts[directory] != section_count:\n",
    "                missing_sections[directory] = unmatched_files\n",
    "\n",
    "            all_legislations.append(legislation_dict)\n",
    "\n",
    "    with open('../COPY_ACT-LEGISLATION_HTML/legislations.json', 'w') as f:\n",
    "        json.dump(all_legislations, f, indent=2)\n",
    "\n",
    "    return missing_sections"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "GREEN = '\\033[92m'\n",
    "RED = '\\033[91m'\n",
    "RESET = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_unique_urls(path):\n",
    "    # Open the json file\n",
    "    with open(path) as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "\n",
    "    url_set = set()\n",
    "\n",
    "    # List to store duplicate sections\n",
    "    duplicate_sections = []\n",
    "\n",
    "    # Iterate through each legislation\n",
    "    for legislation in json_data:\n",
    "\n",
    "        # Iterate through each section in the legislation\n",
    "        for section in legislation['sections']:\n",
    "\n",
    "            # Get the URL of the section\n",
    "            url = section.get('section_url', None)\n",
    "\n",
    "            if url:\n",
    "                # If URL is already in the set, it's not unique, add it to duplicate_sections\n",
    "                if url in url_set:\n",
    "                    print(section)\n",
    "                    duplicate_sections.append(section)\n",
    "                else:\n",
    "                    # If URL is not in the set, add it to the set\n",
    "                    url_set.add(url)\n",
    "\n",
    "    # Write duplicate sections to a JSON file\n",
    "    with open('../2023-05-27__ACT-LEGISLATION_HTML/duplicate_sections.json', 'w') as outfile:\n",
    "        json.dump(duplicate_sections, outfile, indent=4)\n",
    "\n",
    "\n",
    "def group_sections_by_url(file_path):\n",
    "    # Load the data from the JSON file\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Use a defaultdict to easily group data by section_url\n",
    "    grouped_data = defaultdict(list)\n",
    "\n",
    "    # Iterate over each section in data\n",
    "    for section in data:\n",
    "        # Append the section to the list of sections for the corresponding section_url\n",
    "        grouped_data[section[\"section_url\"]].append(section)\n",
    "\n",
    "    # Convert back to a regular dict for serialization\n",
    "    grouped_data = dict(grouped_data)\n",
    "\n",
    "    # Write the grouped data back to the JSON file\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(grouped_data, file, indent=4)\n",
    "\n",
    "\n",
    "def count_sections(data):\n",
    "    count = 0\n",
    "    for legislation in data:\n",
    "        for section in legislation['sections']:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "def count_files(directory):\n",
    "    count = 0\n",
    "    for root, _, files in os.walk(directory):\n",
    "        count += len(files)\n",
    "    return count\n",
    "\n",
    "\n",
    "def count_folders(directory):\n",
    "    count = 0\n",
    "    for _, dirnames, _ in os.walk(directory):\n",
    "        count += len(dirnames)\n",
    "    return count\n",
    "\n",
    "\n",
    "def check_sum(target_path, source_path):\n",
    "    with open(os.path.join(target_path, \"legislations.json\")) as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "\n",
    "    num_legislation_source = count_folders(source_path)\n",
    "    num_legislation_target = count_folders(target_path)\n",
    "    num_legislation_json = len(json_data)\n",
    "    if num_legislation_json == num_legislation_source == num_legislation_target:\n",
    "        print(GREEN+\"Legislation count check passed!\" + RESET)\n",
    "        print(f\"    Number of folders source: {num_legislation_source}\")\n",
    "        print(f\"    Number of folders target: {num_legislation_target}\")\n",
    "        print(f\"    Number of legislations json: {num_legislation_json}\")\n",
    "    else:\n",
    "        print(RED+\"Legislation count check failed!\" + RESET)\n",
    "        print(f\"    Number of folders source: {num_legislation_source}\")\n",
    "        print(f\"    Number of folders target: {num_legislation_target}\")\n",
    "        print(f\"    Number of legislations json: {num_legislation_json}\")\n",
    "\n",
    "    num_files_source = count_files(source_path)\n",
    "    num_files_target = count_files(target_path)\n",
    "    num_sections_json = count_sections(json_data)\n",
    "    if num_sections_json == num_files_source == num_files_target:\n",
    "        print(GREEN+\"Section count check passed!\" + RESET)\n",
    "        print(f\"    Number of files source: {num_files_source}\")\n",
    "        print(f\"    Number of files target: {num_files_target}\")\n",
    "        print(f\"    Number of sections json: {num_sections_json}\")\n",
    "    else:\n",
    "        print(RED+\"Section count check failed!\" + RESET)\n",
    "        print(f\"    Number of files source: {num_files_source}\")\n",
    "        print(f\"    Number of files target: {num_files_target}\")\n",
    "        print(f\"    Number of sections json: {num_sections_json}\")\n",
    "\n",
    "        print(\n",
    "            RED+f\"MISSING {num_files_source - num_files_target} SECTIONS IN TARGET\" + RESET)\n",
    "        print(\n",
    "            RED+f\"MISSING {num_files_source - num_sections_json} SECTIONS IN JSON\" + RESET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-01 20:21:02,758 - ERROR - Could not get year for MAGNA CARTA 1297 25 EDW 1 C 29\n",
      "2023-06-01 20:21:02,895 - ERROR - Could not get year for HEAVY VEHICLE NATIONAL LAW ACT\n",
      "2023-06-01 20:21:03,784 - ERROR - Could not get year for ELECTRONIC CONVEYANCING NATIONAL LAW ACT\n",
      "2023-06-01 20:21:04,001 - ERROR - Could not get year for APPROPRIATION OFFICE OF THE LEGISLATIVE ASSEMBLY ACT 20222023 NO 2\n",
      "2023-06-01 20:21:04,342 - ERROR - Could not get year for APPROPRIATION ACT 20222023 NO 2\n",
      "2023-06-01 20:21:04,348 - ERROR - Could not get year for HEALTH PRACTITIONER REGULATION NATIONAL LAW ACT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'section_name': 'N3yearly investigation by actuary', 'section_order': '79', 'section_url': 'https://storage.googleapis.com/law-docs/ACT-LEGISLATION_HTML/LONG%20SERVICE%20LEAVE%20PORTABLE%20SCHEMES%20ACT%202009/s79.html'}\n",
      "{'section_name': 'S 3500 and Legislation Act s 47', 'section_order': '126a', 'section_url': 'https://storage.googleapis.com/law-docs/ACT-LEGISLATION_HTML/CONSTRUCTION%20OCCUPATIONS%20LICENSING%20ACT%202004/s126a.html'}\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../2023-05-27__ACT-LEGISLATION_HTML/duplicate_sections.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m missing \u001b[39m=\u001b[39m process_all_directories(main_directory)\n\u001b[0;32m----> 2\u001b[0m check_unique_urls(main_directory\u001b[39m+\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m/legislations.json\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      3\u001b[0m check_sum(main_directory, source_directory)\n\u001b[1;32m      5\u001b[0m \u001b[39mprint\u001b[39m(RED\u001b[39m+\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMISSING: \u001b[39m\u001b[39m{\u001b[39;00mmissing\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m RESET)\n",
      "Cell \u001b[0;32mIn[21], line 30\u001b[0m, in \u001b[0;36mcheck_unique_urls\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     27\u001b[0m                 url_set\u001b[39m.\u001b[39madd(url)\n\u001b[1;32m     29\u001b[0m \u001b[39m# Write duplicate sections to a JSON file\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39m../2023-05-27__ACT-LEGISLATION_HTML/duplicate_sections.json\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mw\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m outfile:\n\u001b[1;32m     31\u001b[0m     json\u001b[39m.\u001b[39mdump(duplicate_sections, outfile, indent\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)\n",
      "File \u001b[0;32m~/.virtualenvs/selenium/lib/python3.11/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../2023-05-27__ACT-LEGISLATION_HTML/duplicate_sections.json'"
     ]
    }
   ],
   "source": [
    "\n",
    "missing = process_all_directories(main_directory)\n",
    "check_unique_urls(main_directory+\"/legislations.json\")\n",
    "check_sum(main_directory, source_directory)\n",
    "\n",
    "print(RED+f\"MISSING: {missing}\" + RESET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schedule_pattern = r'SCHEDULE (\\d+)([A-Z]*)\\.html$'\n",
    "schedule_match = re.search(schedule_pattern, \"SCHEDULE 1A.html\")\n",
    "schedule_match.group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_num_pattern = r'SCHEDULE\\s(\\d+)([A-Z]+)'\n",
    "post_num_match = re.search(post_num_pattern, \"SCHEDULE 1A.html\")\n",
    "len(post_num_match.group(2)) if post_num_match else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TEST_extract_section_details(file_name, directory_name):\n",
    "    base_url = \"https://storage.googleapis.com/law-docs/2023-05-27__ACT-LEGISLATION_HTML\"\n",
    "    schedule_pattern = r'SCHEDULE (\\d+)([A-Z]*)\\.html$'\n",
    "    schedule_match = re.search(schedule_pattern, file_name)\n",
    "    if schedule_match:\n",
    "        sched_num = 'sch-' + schedule_match.group(1)\n",
    "        postfix = schedule_match.group(2)\n",
    "        num_of_capital_letters = should_include_postfix(file_name)\n",
    "        if postfix and num_of_capital_letters > 1:\n",
    "            sched_num += postfix[:num_of_capital_letters - 1].lower()\n",
    "            sched_name = 'SCHEDULE ' + schedule_match.group(1) + postfix[-1]\n",
    "        else:\n",
    "            sched_num += postfix.lower()\n",
    "            sched_name = 'SCHEDULE ' + schedule_match.group(1) + postfix\n",
    "        sched_url = f\"{base_url}/{urllib.parse.quote(directory_name)}/{urllib.parse.quote(sched_num)}.html\"\n",
    "        return sched_num, sched_name, sched_url, None\n",
    "    else:\n",
    "        schedule_pattern = r'SCHEDULE\\.html$'\n",
    "        schedule_match = re.search(schedule_pattern, file_name)\n",
    "        if schedule_match:\n",
    "            sched_num = 'sch-0'\n",
    "            sched_name = 'SCHEDULE'\n",
    "            sched_url = f\"{base_url}/{urllib.parse.quote(directory_name)}/{urllib.parse.quote(sched_num)}.html\"\n",
    "            return sched_num, sched_name, sched_url, None\n",
    "        else:\n",
    "            schedule_pattern = r'SCHEDULE\\s(.*)\\.html$'\n",
    "            schedule_match = re.search(schedule_pattern, file_name)\n",
    "            if schedule_match:\n",
    "                sched_num = 'sch-' + schedule_match.group(1)\n",
    "                sched_name = 'SCHEDULE ' + schedule_match.group(1)\n",
    "                sched_url = f\"{base_url}/{urllib.parse.quote(directory_name)}/{urllib.parse.quote(sched_num)}.html\"\n",
    "                return sched_num, sched_name, sched_url, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_unique_urls(\"../2023-05-27__ACT-LEGISLATION_HTML/legislations.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_folders(directory):\n",
    "    # List to store the names of folders\n",
    "    folder_list = []\n",
    "\n",
    "    # Iterate over all the items in the directory\n",
    "    for item in os.listdir(directory):\n",
    "        # Check if the item is a directory\n",
    "        if os.path.isdir(os.path.join(directory, item)):\n",
    "            # Check if the last 4 characters of the folder name are not numeric\n",
    "            if not item[-4:].isnumeric():\n",
    "                # If not, add the folder name to the list\n",
    "                folder_list.append(item)\n",
    "\n",
    "    return folder_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_folders('/Users/home/projects/selenium-py/docs/2023-05-18__ACT-LEGISLATION_HTML')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import codecs\n",
    "\n",
    "def find_article_tags(directory, log_file):\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file == 'combined.html':\n",
    "                filepath = os.path.join(root, file)\n",
    "                with codecs.open(filepath, 'r', 'utf-8') as f:\n",
    "                    soup = BeautifulSoup(f, 'html.parser')\n",
    "                    for article in soup.find_all('article'):\n",
    "                        if 'id' not in article.attrs:\n",
    "                            h2 = article.find('h2')\n",
    "                            if h2:\n",
    "                                with open(log_file, 'a') as log:\n",
    "                                    log.write(f'File Path: {filepath}, h2 content: {h2.text}\\n')\n",
    "\n",
    "# Example usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_article_tags('/Users/home/projects/selenium-py/ACT-LEGISLATION_HTML copy', 'log.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "selenium",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
